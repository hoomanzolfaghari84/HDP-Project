\section{Our Goal}\label{sec:conclusion}

\subsubsection{Problem Definition:}
While NTK provides foundational insights into neural network training dynamics in the infinite-width limit, several challenges persist. Current NTK results are limited to Gaussian or bounded data distributions and specific activation functions, restricting their broader applicability. Additionally, NTK theory primarily addresses fully-connected networks, necessitating extensions to modern architectures like Transformers and CNNs. Furthermore, the influence of NTK on Sample Complexity, Feature Selection, and settings other than Supervised Learning remains inadequately understood.

Building on existing NTK research, our study aims to investigate these topics, leveraging High-Dimensional Probability as the foundational framework to achieve our objectives:

\begin{itemize}
	
	
	\item \textbf{Assess Sample Complexity and Model Capacity:}
	\begin{itemize}
		\item Study explicit sample complexity, VC-Dimension, and Rademacher Complexity in NTK models.
	\end{itemize}
	
	\item \textbf{Broaden NTK to Diverse Data Distributions:}
	\begin{itemize}
		\item Extend current results depending on Gaussian/bounded to sub-Gaussian/sub-Exponential distributions.
		\item Examine how data distribution affects NTK convergence and generalization.
	\end{itemize}
	
	\item \textbf{Investigate Feature Selection Properties of NTK:}
	\begin{itemize}
		\item Analyze NTK's spectral distribution for feature learning and bias.
		
		\item Examine how NTK models memorize spurious data and factors influencing this behavior.
		
		\item Develop methods to reduce overfitting to irrelevant data within the NTK framework.
		
		\item Conduct sensitivity analyses to assess model robustness to input changes.
	\end{itemize}

\item \textbf{Expand NTK to Advanced Architectures:} For example Transformers and Kolmogorov-Arnold Networks

\item Extend NTK to Unsupervised/Self-Supervised/Transfer Learning settings.
\end{itemize}